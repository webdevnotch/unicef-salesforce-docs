# High-Volume Queue Setup for 450,000 Jobs/Day

This setup is optimized to handle **450,000+ jobs per day** with high performance and reliability.

## ðŸš€ Performance Specifications

- **Capacity**: 450,000+ jobs/day (18,750 jobs/hour)
- **Peak Load**: 5+ jobs/second sustained
- **Processing Time**: <2 seconds per job average
- **Error Rate**: <1% with proper retry logic
- **Queue Depth**: <100 jobs under normal load

## ðŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Main App      â”‚    â”‚   Redis Queue    â”‚    â”‚  Batch Processorâ”‚
â”‚   (NestJS API)  â”‚â”€â”€â”€â–¶â”‚   (BullMQ)       â”‚â”€â”€â”€â–¶â”‚  (High Volume)  â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ API Endpoints â”‚    â”‚ â€¢ Job Storage    â”‚    â”‚ â€¢ Batch DB Ops  â”‚
â”‚ â€¢ Job Schedulingâ”‚    â”‚ â€¢ Queue Logic    â”‚    â”‚ â€¢ Performance   â”‚
â”‚ â€¢ Monitoring    â”‚    â”‚ â€¢ High Concurrencyâ”‚   â”‚ â€¢ Auto-scaling  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“Š Key Features

### 1. **Batch Database Operations**
- **Batch Size**: 100 operations per batch
- **Batch Timeout**: 5 seconds
- **Performance**: 10x faster than individual updates
- **Memory Efficient**: Prevents memory leaks

### 2. **High-Performance Queue Configuration**
- **Concurrency**: 20 workers per queue
- **Retry Logic**: Smart retry with exponential backoff
- **Memory Management**: Optimized for high volume
- **Stall Detection**: 10-second timeout

### 3. **Advanced Monitoring**
- **Real-time Metrics**: Jobs/second, queue depth, error rates
- **Performance Alerts**: Automatic alerting on thresholds
- **Health Checks**: Comprehensive system health monitoring
- **Dashboard**: Web-based monitoring interface

### 4. **Database Optimizations**
- **Indexes**: 15+ optimized indexes for high volume
- **Connection Pooling**: 20 connections per instance
- **Query Optimization**: Batch operations and prepared statements
- **Auto-vacuum**: Automated database maintenance

## ðŸ› ï¸ Setup Instructions

### 1. **Environment Variables**
```bash
# Required environment variables
export DATABASE_URL="postgresql://user:pass@localhost:5432/db"
export REDIS_URL="redis://localhost:6379"
export SF_BASE_ENDPOINT="https://your-salesforce-endpoint.com"
export SF_CLIENT_ID="your-client-id"
export SF_CLIENT_SECRET="your-client-secret"
```

### 2. **Quick Start**
```bash
# Make setup script executable
chmod +x scripts/init-high-volume.sh

# Run initialization
./scripts/init-high-volume.sh
```

### 3. **Manual Setup**
```bash
# Install dependencies
npm install

# Create database indexes
npx prisma db execute --file ./scripts/create-indexes.sql

# Run database optimizations
npx prisma db execute --file ./scripts/database-optimizations.sql

# Start services
docker-compose -f docker-compose.high-volume.yml up -d
```

## ðŸ“ˆ Monitoring Endpoints

### **Health Check**
```bash
curl http://localhost:3000/queue/monitor/health
```

### **Detailed Metrics**
```bash
curl http://localhost:3000/queue/monitor/detailed
```

### **Performance Metrics**
```bash
curl http://localhost:3000/queue/monitor/metrics
```

### **Force Batch Flush**
```bash
curl -X POST http://localhost:3000/queue/monitor/force-flush
```

## ðŸ”§ Configuration

### **Queue Settings**
```typescript
// High-performance configuration
const config = {
  concurrency: 20,           // Workers per queue
  batchSize: 100,         // Batch operations
  maxAttempts: 2,         // Retry attempts
  retryDelay: 500,       // Retry delay (ms)
  stalledInterval: 10000, // Stall detection (ms)
};
```

### **Database Settings**
```sql
-- High-performance PostgreSQL settings
max_connections = 200
shared_buffers = 256MB
effective_cache_size = 1GB
work_mem = 4MB
maintenance_work_mem = 64MB
```

### **Redis Settings**
```ini
# High-performance Redis settings
maxmemory 1gb
maxmemory-policy allkeys-lru
tcp-keepalive 60
timeout 300
```

## ðŸ“Š Performance Monitoring

### **Key Metrics**
- **Jobs/Second**: Current processing rate
- **Queue Depth**: Number of pending jobs
- **Error Rate**: Percentage of failed jobs
- **Processing Time**: Average job processing time
- **Memory Usage**: System memory utilization
- **CPU Usage**: System CPU utilization

### **Alert Thresholds**
- **Queue Depth**: >5,000 jobs waiting
- **Error Rate**: >5% failure rate
- **Processing Time**: >10 seconds average
- **Memory Usage**: >80% utilization

## ðŸš¨ Troubleshooting

### **High Queue Depth**
```bash
# Check queue status
curl http://localhost:3000/queue/monitor/health

# Force batch flush
curl -X POST http://localhost:3000/queue/monitor/force-flush
```

### **High Error Rate**
```bash
# Check error details
curl http://localhost:3000/queue/monitor/alerts

# Review logs
docker-compose logs app
```

### **Performance Issues**
```bash
# Check database performance
curl http://localhost:3000/queue/monitor/detailed

# Review system resources
docker stats
```

## ðŸ”„ Scaling

### **Horizontal Scaling**
```bash
# Scale application instances
docker-compose -f docker-compose.high-volume.yml up -d --scale app=4

# Scale Redis cluster
docker-compose -f docker-compose.high-volume.yml up -d --scale redis-cluster=3
```

### **Vertical Scaling**
```yaml
# Increase resources in docker-compose.high-volume.yml
deploy:
  resources:
    limits:
      memory: 4G      # Increase from 2G
      cpus: '2.0'    # Increase from 1.0
```

## ðŸ“‹ Maintenance

### **Daily Tasks**
- Monitor queue depth and error rates
- Check system resource utilization
- Review performance metrics

### **Weekly Tasks**
- Database vacuum and analyze
- Review and optimize slow queries
- Update monitoring dashboards

### **Monthly Tasks**
- Performance review and optimization
- Capacity planning
- Security updates

## ðŸŽ¯ Expected Performance

With this setup, you should achieve:
- **450,000+ jobs/day** sustained capacity
- **<2 second** average processing time
- **<1%** error rate
- **<100 jobs** queue depth under normal load
- **99.9%** uptime

## ðŸ“ž Support

For issues or questions:
1. Check the monitoring endpoints
2. Review the logs: `docker-compose logs app`
3. Check system resources: `docker stats`
4. Review database performance: `curl http://localhost:3000/queue/monitor/detailed`
